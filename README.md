# Coinbase Kafka Spark Cassandra Pipeline

This project implements a **real-time cryptocurrency data pipeline** using **Airflow, Kafka, Spark Streaming, and Cassandra**.  
It streams live trades from **CoinBase**, processes them in real time, and stores enriched results for further analysis.

---

## ðŸ“Œ Summary  

**Airflow orchestrates â†’ WebSocket streams CoinBase trades â†’ Kafka transports â†’ Spark processes & computes profits â†’ Cassandra stores results.**

---

<img src="https://github.com/user-attachments/assets/5fc3f03e-3d05-4331-9b78-66171cbc99ef" alt="CoinBase_Kafka_Spark_Airflow_Cassandra" style="width:600px;"/>



## ðŸš€ Workflow Overview  

1. **Triggering the DAG (Airflow)**  
   - Airflow runs the DAG (`Crypto_Kafka_Stream.py`).  
   - DAG starts a Python task that connects to the **CoinBase WebSocket API**.

<img src="https://github.com/user-attachments/assets/19259b1c-1749-4f15-9fff-ee7a791e4fab" 
     alt="project2Airflow" 
     width="600" 
     height="200"/>

2. **Data Ingestion (WebSocket â†’ Kafka)**  
   - The WebSocket subscribes only to cryptocurrencies listed in `my_Portfolio.json`.  
   - `coin_map.json` is used for symbol-to-name lookup.  
   - Trade messages are transformed and published into a Kafka topic (`crypto_trades`).  

3. **Stream Processing (Kafka â†’ Spark)**  
   - Spark Structured Streaming consumes data from Kafka.  
   - Data is parsed, cast into proper schema, and enriched with **profit calculation** based on portfolio buy prices & sizes.  

4. **Storage (Spark â†’ Cassandra)**  
   - Processed data is written into a **Cassandra keyspace (`crypto_keyspace`)** and table (`crypto_trades`).  
   - Each row contains symbol, event time, trade size, price, side, and computed profit.
  
     <img width="600" height="200" alt="cassandraProfit" src="https://github.com/user-attachments/assets/36a9166f-1afc-4eac-90be-cd236a775aea" />


---

## Tech Stack  

- **Apache Airflow** â†’ Workflow orchestration  
- **CoinBase WebSocket API** â†’ Real-time trade data source  
- **Apache Kafka** â†’ Message streaming & buffering  
- **Apache Spark (Structured Streaming)** â†’ Real-time data processing & profit computation  
- **Apache Cassandra** â†’ Scalable storage for enriched trades  
- **Docker Compose** â†’ Containerized deployment of the entire stack  

---

## ðŸ“‚ Project Structure  

```plaintext
.
â”œâ”€â”€ airflow
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”œâ”€â”€ dags
â”‚   â”‚   â””â”€â”€ Crypto_Kafka_Stream.py
â”‚   â””â”€â”€ script
â”‚       â””â”€â”€ entrypoint.sh
â”œâ”€â”€ required_data
â”‚   â”œâ”€â”€ coin_map.json
â”‚   â””â”€â”€ my_Portfolio.json
â”œâ”€â”€ spark
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â””â”€â”€ spark_streaming.py
â”œâ”€â”€ docker-compose.yaml
â”œâ”€â”€ README.md
â””â”€â”€ requirements.txt
